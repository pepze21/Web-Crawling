{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3288dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# log in to twitter\n",
    "url = 'https://twitter.com/login'\n",
    "webbrowser.open(url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be20f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#https://twitter.com/search?lang=ko&q=(%23%EB%B8%94%EB%9E%99%ED%95%91%ED%81%AC)&src=typed_query # 해시태그 블랙핑크로 검색\n",
    "    \n",
    "#https://twitter.com/search?q=210813%20(%23%EB%B8%94%EB%9E%99%ED%95%91%ED%81%AC)&src=typed_query&f=image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4ea091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787852d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8c596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ecce7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div><span> <a href=\"http://www.naver.com\">naver</a> <a href=\"https://www.google.com\">google</a> <a href=\"http://www.daum.net/\">daum</a> </span></div></body></html>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 html 코드\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>\"\"\"\n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "653db6a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfef10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파싱 뜻 = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18aebe25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <div>\n",
      "   <span>\n",
      "    <a href=\"http://www.naver.com\">\n",
      "     naver\n",
      "    </a>\n",
      "    <a href=\"https://www.google.com\">\n",
      "     google\n",
      "    </a>\n",
      "    <a href=\"http://www.daum.net/\">\n",
      "     daum\n",
      "    </a>\n",
      "   </span>\n",
      "  </div>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69b742e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"http://www.naver.com\">naver</a>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a') #처음 1개만 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f713c4de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9126f559522e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2172\u001b[0m         \u001b[1;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2173\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m   2174\u001b[0m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'get_text'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "soup.find('a').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e64f1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"http://www.naver.com\">naver</a>,\n",
       " <a href=\"https://www.google.com\">google</a>,\n",
       " <a href=\"http://www.daum.net/\">daum</a>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a') #모두 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf3427f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver\n",
      "google\n",
      "daum\n"
     ]
    }
   ],
   "source": [
    "site_names = soup.find_all('a')\n",
    "for site_name in site_names:\n",
    "    print(site_name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f94a53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://www.naver.com\">naver</a>\n",
      "<a href=\"http://www.naver.com\">naver</a>\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('a'))\n",
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "81817bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .find('a')로 해도 되고, .a로 해도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "101f41bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://www.naver.com\">naver</a>\n"
     ]
    }
   ],
   "source": [
    "print(soup.body.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7bc4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .body.a 처럼 이어서 쓰는것도 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31b02723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>작품과 작가 모음</title>\n",
       "</head>\n",
       "<body>\n",
       "<h1>책 정보</h1>\n",
       "<p id=\"book_title\">토지</p>\n",
       "<p id=\"auther\">박경리</p>\n",
       "<p id=\"book_title\">태백산맥</p>\n",
       "<p id=\"auther\">조정래</p>\n",
       "<p id=\"book_title\">감옥으로부터의 사색</p>\n",
       "<p id=\"auther\">신영복</p>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 html 코드\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <title>작품과 작가 모음</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>책 정보</h1>\n",
    "  <p id=\"book_title\">토지</p>\n",
    "  <p id=\"auther\">박경리</p>\n",
    "  \n",
    "  <p id=\"book_title\">태백산맥</p>\n",
    "  <p id=\"auther\">조정래</p>\n",
    "  \n",
    "  <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    "  <p id=\"auther\">신영복</p>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup2 = BeautifulSoup(html2, 'lxml')\n",
    "soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e494824e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-ab20f3934491>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "# 여기서 원래는 id는 unique해야 됨. 그런데 html은 포용력이 넓어서\n",
    "# 이렇게 좀 이상하게 만들어도 잘 작동함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f6b119c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   작품과 작가 모음\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   책 정보\n",
      "  </h1>\n",
      "  <p id=\"book_title\">\n",
      "   토지\n",
      "  </p>\n",
      "  <p id=\"auther\">\n",
      "   박경리\n",
      "  </p>\n",
      "  <p id=\"book_title\">\n",
      "   태백산맥\n",
      "  </p>\n",
      "  <p id=\"auther\">\n",
      "   조정래\n",
      "  </p>\n",
      "  <p id=\"book_title\">\n",
      "   감옥으로부터의 사색\n",
      "  </p>\n",
      "  <p id=\"auther\">\n",
      "   신영복\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup2.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd591386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>작품과 작가 모음</title>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09120e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbada21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eebe48c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n <head>\\n  <title>\\n   작품과 작가 모음\\n  </title>\\n </head>\\n <body>\\n  <h1>\\n   책 정보\\n  </h1>\\n  <p id=\"book_title\">\\n   토지\\n  </p>\\n  <p id=\"auther\">\\n   박경리\\n  </p>\\n  <p id=\"book_title\">\\n   태백산맥\\n  </p>\\n  <p id=\"auther\">\\n   조정래\\n  </p>\\n  <p id=\"book_title\">\\n   감옥으로부터의 사색\\n  </p>\\n  <p id=\"auther\">\\n   신영복\\n  </p>\\n </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup2.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "051f5138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72e3091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "\n",
      "a\n",
      "\n",
      "bc d\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f611645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\na\\n\\nbc d'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##여기부터는 진짜 개인적인 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5adaec8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'cp949' codec can't decode byte 0xeb in position 368: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-59a999ed59e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'naver_main.html'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msoup1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lxml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'read'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m        \u001b[1;31m# It's a file-type object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m         elif len(markup) <= 256 and (\n\u001b[0;32m    311\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34mb'<'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'cp949' codec can't decode byte 0xeb in position 368: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('naver_main.html') as f:\n",
    "    soup1 = BeautifulSoup(f, 'lxml')\n",
    "\n",
    "print(soup1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5436cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4939a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Top Sites in South Korea]\n",
      "1: Google.com\n",
      "2: Naver.com\n",
      "3: Youtube.com\n",
      "4: Daum.net\n",
      "5: Tistory.com\n",
      "6: Kakao.com\n",
      "[Top Sites in South Korea]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naver.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Youtube.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Daum.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tistory.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kakao.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Website\n",
       "1   Google.com\n",
       "2    Naver.com\n",
       "3  Youtube.com\n",
       "4     Daum.net\n",
       "5  Tistory.com\n",
       "6    Kakao.com"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.alexa.com/topsites/countries/KR\"\n",
    "\n",
    "html_website_ranking = requests.get(url).text\n",
    "soup_website_ranking = BeautifulSoup(html_website_ranking, \"lxml\")\n",
    "\n",
    "\n",
    "# p 태그의 요소 안에서 a 태그의 요소를 찾음\n",
    "website_ranking = soup_website_ranking.select('p a')\n",
    "website_ranking_address = [website_ranking_element.get_text() for website_ranking_element in website_ranking[1:]]\n",
    "\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "for k in range(6):\n",
    "    print(\"{0}: {1}\".format(k+1, website_ranking_address[k]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "website_ranking_dict = {'Website' : website_ranking_address}\n",
    "df = pd.DataFrame(website_ranking_dict)\n",
    "df.index=df.index+1\n",
    "print(\"[Top Sites in South Korea]\")\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229d551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a52395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 웹크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "774b07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "선택한 모든 이미지 내려받기 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "#URL을 받아서, 해당 URL에 존재하는 img URL들을 list로 반환하는 함수\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "#폴더를 지정해 이미지 주소에서 이미지 내려받기\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)은 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "        \n",
    "        chunk_size = 1000000 # 이미지 데이터를 1000000bytes씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "    else:\n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "#웹 사이트의 주소 지정\n",
    "reshot_url = 'https://www.reshot.com/search/animal'\n",
    "\n",
    "figure_folder = \"C:/Users/2020/WebCrawlingTest\" # 이미지를 내려받을 폴더 지정\n",
    "if not os.path.exists(figure_folder):\n",
    "    os.makedirs(figure_folder)\n",
    "\n",
    "\n",
    "reshot_image_urls = get_image_url(reshot_url) #이미지 파일의 주소 가져오기\n",
    "\n",
    "num_of_download_image = 7 # 내려받을 이미지 개수 지정\n",
    "# num_of_download_image = len(reshot_image_urls)\n",
    "\n",
    "for k in range(1, num_of_download_image+1):\n",
    "    download_image(figure_folder, reshot_image_urls[k])\n",
    "print(\"=====================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter에서 가져와보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b9b489b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-b989531296c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_download_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mdownload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigure_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwitter_image_urls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"=====================================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"선택한 모든 이미지 내려받기 완료!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def get_image_url(url):\n",
    "    html_image_url = requests.get(url).text\n",
    "    soup_image_url = BeautifulSoup(html_image_url, \"lxml\")\n",
    "    image_elements = soup_image_url.select('img')\n",
    "    if(image_elements != None):\n",
    "        image_urls = []\n",
    "        for image_element in image_elements:\n",
    "            image_urls.append(image_element.get('src'))\n",
    "        return image_urls\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def download_image(img_folder, img_url):\n",
    "    if(img_url != None):\n",
    "        html_image = requests.get(img_url)\n",
    "        # os.path.basename(URL)은 웹사이트나 폴더가 포함된 파일명에서 파일명만 분리\n",
    "        imageFile = open(os.path.join(img_folder, os.path.basename(img_url)), 'wb')\n",
    "        \n",
    "        chunk_size = 1000000 # 이미지 데이터를 1000000bytes씩 나눠서 저장\n",
    "        for chunk in html_image.iter_content(chunk_size):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "    else:\n",
    "        print(\"내려받을 이미지가 없습니다.\")\n",
    "        \n",
    "twitter_url = 'https://twitter.com/search?q=(%23%EC%95%84%EB%A6%B0)&src=typed_query&f=image'\n",
    "\n",
    "\n",
    "figure_folder = \"C:/Users/2020/WebCrawlingTest\" # 이미지를 내려받을 폴더 지정\n",
    "if not os.path.exists(figure_folder):\n",
    "    os.makedirs(figure_folder)\n",
    "\n",
    "\n",
    "twitter_image_urls = get_image_url(twitter_url) #이미지 파일의 주소 가져오기\n",
    "num_of_download_image = 2 # 내려받을 이미지 개수 지정\n",
    "\n",
    "for k in range(num_of_download_image):\n",
    "    download_image(figure_folder, twitter_image_urls[k])\n",
    "print(\"=====================================\")\n",
    "print(\"선택한 모든 이미지 내려받기 완료!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b41d3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db370347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "html_image = requests.get('https://pbs.twimg.com/media/E8qdVROVIAQEFFi?format=jpg&name=large')\n",
    "type(html_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a2209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div aria-label=\"이미지\" class=\"css-1dbjc4n r-1p0dtai r-1mlwlqe r-1d2f490 r-11wrixw r-61z16t r-1mnahxq r-1udh08x r-u8s1d r-zchlnj r-ipm5af r-417010\" data-testid=\"tweetPhoto\" style=\"margin-bottom: -22%;\"><div class=\"css-1dbjc4n r-1niwhzg r-vvn4in r-u6sd8q r-4gszlv r-1p0dtai r-1pi2tsx r-1d2f490 r-u8s1d r-zchlnj r-ipm5af r-13qz1uu r-1wyyakw\" style=\"background-image: url(&quot;https://pbs.twimg.com/media/E8_buC_VEAUpann?format=jpg&amp;name=small&quot;);\"></div><img alt=\"이미지\" draggable=\"true\" src=\"https://pbs.twimg.com/media/E8_buC_VEAUpann?format=jpg&amp;name=small\" class=\"css-9pa8cd\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d71972",
   "metadata": {},
   "outputs": [],
   "source": [
    "<img alt=\"이미지\" draggable=\"true\" src=\"https://pbs.twimg.com/media/E8_buC_VEAUpann?format=jpg&amp;name=small\" class=\"css-9pa8cd\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b65f6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae051d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "https://twitter.com/search?q=210819(%23%EC%95%84%EB%A6%B0)&src=typed_query&f=image\n",
    "        https://twitter.com/search?q=(%23%EC%95%84%EB%A6%B0)&src=typed_query&f=image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b380a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6fb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6429b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43395e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d9f76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n",
      "Installing collected packages: selenium\n",
      "Successfully installed selenium-3.141.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "546e67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c270e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/Users/2020/chromedriver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51f3ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://www.naver.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4575d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://www.youtube.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad322228",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.set_window_size(1024, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d3b4474",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(200, 300);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "86596913",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"alert('selenium test');\") # alert 창 띄움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fb3a8478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selenium test\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    alert = driver.switch_to.alert\n",
    "    print(alert.text)\n",
    "except:\n",
    "    print('alert 없음')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d6a1021",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert.accept() # alert 창 확인버튼 클릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "12c50685",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector(\".btn_submit\").click() # 검색 버튼 클릭방법1(.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "065c86c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector('#search_btn').click() # 검색 버튼 클릭방법2(#id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "86184c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.ted.com/talks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f6bdd8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Join TED Recommends to get the best ideas, selected just for you'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element_by_css_selector('#banner-secondary').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "12f86409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = driver.find_elements_by_css_selector('#browse-results > div > .col')\n",
    "len(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d0bd9e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The secret society of the Great Dismal Swamp'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0].find_element_by_css_selector('.media > .media__message .ga-link').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db7ac96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The secret society of the Great Dismal Swamp',\n",
       " 'Walk with Little Amal, a theatrical journey celebrating the refugee experience',\n",
       " 'How COVID-19 reshaped US cities',\n",
       " 'Can you solve the giant spider riddle?',\n",
       " 'An interactive map to track (and end) pollution in China',\n",
       " 'Why are stolen African artifacts still in Western museums?',\n",
       " 'The fingerprints of life beyond Earth',\n",
       " 'The method that can \"prove\" almost anything',\n",
       " 'The tiny balls of fat that could revolutionize medicine',\n",
       " \"Don't call people out — call them in\",\n",
       " 'What causes seizures, and how can we treat them?',\n",
       " 'Meet the scientist couple driving an mRNA vaccine revolution',\n",
       " 'Remembering climate change ... a message from the year 2071',\n",
       " 'The informal settlements reshaping the world',\n",
       " \"Iceland's superpowered underground volcanoes\",\n",
       " 'The radical, revolutionary resilience of Black joy',\n",
       " \"A year in the life of one of Earth's weirdest animals\",\n",
       " '3 rules for a zero-carbon world',\n",
       " 'The ancient origins of the Olympics',\n",
       " 'The rise and fall of the Kingdom of Man',\n",
       " 'Are wild animals really \"wild\"?',\n",
       " 'How every child can thrive by five',\n",
       " \"The paradox at the heart of mathematics: Gödel's Incompleteness Theorem\",\n",
       " 'The (de)colonizing of beauty',\n",
       " 'Why COP26 is our best chance for a greener future',\n",
       " '3 myths about racism that keep the US from progress',\n",
       " 'How do governments create money out of thin air?',\n",
       " 'The missing 96 percent of the universe',\n",
       " 'The most notorious scientific feud in history',\n",
       " 'The link between menopause and gender inequity at work',\n",
       " 'Documentary films that explore trauma — and make space for healing',\n",
       " \"History's deadliest king\",\n",
       " \"A cleanse won't detox your body — but here's what will\",\n",
       " 'What should humans take to space (and leave behind)?',\n",
       " 'Could we build a wooden skyscraper?',\n",
       " \"Why aren't there more Native American restaurants?\"]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = []\n",
    "for content in contents:\n",
    "    title = content.find_element_by_css_selector('.media > .media__message .ga-link').text\n",
    "    titles.append(title)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d740f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English', 'Español', '日本語', 'Português brasileiro', '中文 (繁體)', '한국어']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages = driver.find_element_by_css_selector('#languages').text\n",
    "languages = languages.split(\"\\n\")[1:-1]\n",
    "languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2c612f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector('#languages [lang=\"ko\"]').click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beaa955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "contents = driver.find_elements_by_css_selector('#browse-results > div > .col')\n",
    "titles = []\n",
    "for content in contents:\n",
    "    title = content.find_element_by_css_selector('.media > .media__message .ga-link').text\n",
    "    titles.append(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f6569b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-105-b2140a7b9b1b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-105-b2140a7b9b1b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    $0\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20186a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "contents = driver.find_elements_by_css_selector('#browse-results > div > .col')\n",
    "links = []\n",
    "for content in contents:\n",
    "    link = content.find_element_by_css_selector('.media > .media__message .ga-link').get_attribute('href')\n",
    "    links.append(link)\n",
    "links[-5:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824ea00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "82d29fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-5//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "575bc70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 3]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = [1, 2, 3]\n",
    "list[1] =4\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b93d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
